{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a67fd-5080-43d3-b473-c8b89b9123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, greatest, sum as spark_sum, mean, count,substring, lpad, create_map, expr,trim,round\n",
    "from functools import reduce\n",
    "from MHI.MHI_FORMATS import *\n",
    "from itertools import chain\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "import time\n",
    "import argparse\n",
    "import concurrent.futures\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791b5b0-f53e-47b9-aa16-0b42c58b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--inFile\", help=\"Input data path\", default='/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/sid_2021_10K.csv')\n",
    "#parser.add_argument(\"--refFile\", help=\"Input data path\", default='/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/MHAO_v2024_21_10K.TXT')\n",
    "#args, unknown = parser.parse_known_args()\n",
    "\n",
    "#data_path = args.inFile\n",
    "#sas_result_file = args.refFile\n",
    "\n",
    "data_path = sys.argv[1] #\"D:\\\\skhatiwada\\\\LogicExecutor\\\\Data\\\\sid_2021_100K.csv\"\n",
    "output_path = sys.argv[2] #\"D:\\\\skhatiwada\\\\LogicExecutor\\\\PySpark\\\\Output\\\\sid_2021_100K\"\n",
    "execution_time_output_path = sys.argv[3] #\"D:\\\\skhatiwada\\\\LogicExecutor\\\\PySpark\\\\Output\\\\sid_2021_100K_Execution.txt\"\n",
    "\n",
    "#if os.path.exists(data_path) == False:\n",
    "    #sys.exit('Data file path does not exist')\n",
    "    \n",
    "# if os.path.exists(sas_result_file) == False:\n",
    "#     sys.exit('Ref file path does not exist')\n",
    "    \n",
    "key_columns = [\"PAYCAT\", \"YEAR\", \"HOSPST\", \"POVCAT\", \"RACECAT\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608d98c-452d-4de2-a7b1-bb41882c27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MHI_AREA\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sc.setLogLevel(\"WARN\") \n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "sc.setLogLevel(\"FATAL\")\n",
    "print(\"Log Level Set to FATAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df7bbf-787b-433d-8370-f6b5550f9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Load input data\n",
    "#data_size = '8M'#'10K'\n",
    "#path: /Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/sid_2021_\n",
    "#input_data = spark.read.csv(f\"{data_path}/sid_2021_{data_size}.csv\", header=True, inferSchema=True)\n",
    "input_data = spark.read.csv(f\"{data_path}\", header=True, inferSchema=True)\n",
    "# Check for the presence of PAY1 and RACE columns\n",
    "columns = input_data.columns\n",
    "pay1_provided = \"PAY1\" in columns\n",
    "race_provided = \"RACE\" in columns\n",
    "\n",
    "if not pay1_provided:\n",
    "    print(\"WARNING: The input data does not have PAY1. The software creates a fake PAY1 as PAY1=999 for the programs to run\")\n",
    "if not race_provided:\n",
    "    print(\"WARNING: The input data does not have RACE. The software creates a fake RACE as RACE=999 for the programs to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6b8ff-4c1b-4540-b995-44167c4f4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_columns = [c for c in input_data.columns if c.startswith(\"DX\") and c[2:].isdigit()]\n",
    "pr_columns = [c for c in input_data.columns if c.startswith(\"PR\") and c[2:].isdigit()]\n",
    "\n",
    "def mdx(df: DataFrame, column: str, fmt_list: set) -> DataFrame:\n",
    "    return df.withColumn(column, when(reduce(lambda a, b: a | b, [col(c).isNotNull() & col(c).isin(fmt_list) for c in dx_columns]), 1).otherwise(0))\n",
    "\n",
    "def mpr(df: DataFrame, column: str, fmt_list: set) -> DataFrame:\n",
    "    return df.withColumn(column, when(reduce(lambda a, b: a | b, [col(c).isNotNull() & col(c).isin(fmt_list) for c in pr_columns]), 1).otherwise(0))\n",
    "\n",
    "def create_fake_pay1_race(df, pay1_provided, race_provided):\n",
    "    if not pay1_provided:\n",
    "        df = df.withColumn(\"PAY1\", lit(999))\n",
    "    if not race_provided:\n",
    "        df = df.withColumn(\"RACE\", lit(999))\n",
    "    return df\n",
    "\n",
    "print(f\"Total Execution Time Part 1: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc598a23-56ba-41e1-bd89-d478a7024291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "input_data = input_data.withColumn(\"ICDVER\", \n",
    "    when((col(\"YEAR\") == 2015) & (col(\"DQTR\") == 4), 33)\n",
    "    .when((col(\"YEAR\") == 2016) & (col(\"DQTR\").isin(1, 2, 3)), 33)\n",
    "    .when((col(\"YEAR\") == 2016) & (col(\"DQTR\") == 4), 34)\n",
    "    .when((col(\"YEAR\") == 2017) & (col(\"DQTR\").isin(1, 2, 3)), 34)\n",
    "    .when((col(\"YEAR\") == 2017) & (col(\"DQTR\") == 4), 35)\n",
    "    .when((col(\"YEAR\") == 2018) & (col(\"DQTR\").isin(1, 2, 3)), 35)\n",
    "    .when((col(\"YEAR\") == 2018) & (col(\"DQTR\") == 4), 36)\n",
    "    .when((col(\"YEAR\") == 2019) & (col(\"DQTR\").isin(1, 2, 3)), 36)\n",
    "    .when((col(\"YEAR\") == 2019) & (col(\"DQTR\") == 4), 37)\n",
    "    .when((col(\"YEAR\") == 2020) & (col(\"DQTR\").isin(1, 2, 3)), 37)\n",
    "    .when((col(\"YEAR\") == 2020) & (col(\"DQTR\") == 4), 38)\n",
    "    .when((col(\"YEAR\") == 2021) & (col(\"DQTR\").isin(1, 2, 3)), 38)\n",
    "    .when((col(\"YEAR\") == 2021) & (col(\"DQTR\") == 4), 39)\n",
    "    .when((col(\"YEAR\") == 2022) & (col(\"DQTR\").isin(1, 2, 3)), 39)\n",
    "    .when((col(\"YEAR\") == 2022) & (col(\"DQTR\") == 4), 40)\n",
    "    .when((col(\"YEAR\") == 2023) & (col(\"DQTR\").isin(1, 2, 3)), 40)\n",
    "    .when((col(\"YEAR\") == 2023) & (col(\"DQTR\") == 4), 41)\n",
    "    .when((col(\"YEAR\") == 2024) & (col(\"DQTR\").isin(1, 2, 3)), 41)\n",
    "    .otherwise(41)\n",
    ")\n",
    "\n",
    "# Create fake PAY1 and RACE if they are not in the input data\n",
    "input_data = create_fake_pay1_race(input_data, pay1_provided, race_provided)\n",
    "\n",
    "# Filter out records based on conditions\n",
    "filtered_data = input_data.filter(\n",
    "    (col(\"AGE\") >= 12) & (col(\"AGE\") <= 55) &\n",
    "    col(\"SEX\").isNotNull() &\n",
    "    col(\"DX1\").isNotNull() &\n",
    "    col(\"DQTR\").isNotNull() &\n",
    "    col(\"YEAR\").isNotNull()\n",
    ")\n",
    "\n",
    "#add columns AGECAT, RACECAT, PAYCAT, SEXCAT\n",
    "filtered_data = filtered_data.withColumn(\"PAYCAT\",\n",
    "    when(col(\"PAY1\") == 1, 1)\n",
    "    .when(col(\"PAY1\") == 2, 2)\n",
    "    .when(col(\"PAY1\") == 3, 3)\n",
    "    .when(col(\"PAY1\") == 4, 4)\n",
    "    .when(col(\"PAY1\") == 5, 5)\n",
    "    .when(col(\"PAY1\") == 5, 5)\n",
    "    .otherwise(6)\n",
    ")\n",
    "\n",
    "filtered_data = filtered_data.withColumn(\"RACECAT\",\n",
    "    when(col(\"RACE\") == 1, 1)\n",
    "    .when(col(\"RACE\") == 2, 2)\n",
    "    .when(col(\"RACE\") == 3, 3)\n",
    "    .when(col(\"RACE\") == 4, 4)\n",
    "    .when(col(\"RACE\") == 5, 5)\n",
    "    .otherwise(6)\n",
    ")\n",
    "\n",
    "filtered_data = filtered_data.withColumn(\"AGECAT\",\n",
    "    when((col(\"AGE\") < 18), 0)\n",
    "    .when((col(\"AGE\") >= 18) & (col(\"AGE\") < 40), 1)\n",
    "    .when((col(\"AGE\") >= 40) & (col(\"AGE\") < 65), 2)\n",
    "    .when((col(\"AGE\") >= 65) & (col(\"AGE\") < 75), 3)\n",
    "    .when(col(\"AGE\") >= 75, 4)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "filtered_data = filtered_data.withColumn(\"SEXCAT\",\n",
    "    when(col(\"SEX\") == 1, 1)\n",
    "    .when(col(\"SEX\") == 2, 2)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "print(f\"Total Execution Time Part 2: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856bf30-bc4f-4a38-880a-58b2cef4a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# KEY HOSPID YEAR DQTR AGE SEX\n",
    "#               AGECAT SEXCAT PAYCAT RACECAT POVCAT LOS HOSPST\n",
    "# \t\t\t  MYOCARD ANEURYSM ACUTE_RENAL RESP_DISTRESS AM_FLUID CARD_ARR CONV_CARD DISS_INTRA ECLAMPSIA \n",
    "# \t\t\t  HEART_FAIL PUERP PULM_ED ANES_COMP SEPSIS SHOCK SICKLE AIR_THROMB HYSTER TRACHEO VENT\n",
    "# \t\t\t  MATH_ABORT DECEASED_FLAG ACUTE_RENAL3 DISS_INTRA3\n",
    "#               TAMH01 TAMH02 TAMH03\n",
    "# Calculate indicator\n",
    "# Convert dictionary to PySpark mapping\n",
    "mapping_expr = create_map([lit(x) for x in chain(*POVCAT.items())])\n",
    "# Apply functions\n",
    "filtered_data = mdx(filtered_data, \"deliv_dx\", DX_Delivery_Keys)\n",
    "filtered_data = mpr(filtered_data, \"deliv_pr\", PR_Delivery_Keys)\n",
    "\n",
    "#keep rows that has deliv_dx or deliv_pr 1\n",
    "filtered_data = filtered_data.filter((col(\"deliv_dx\") == 1) | (col(\"deliv_pr\") == 1))\n",
    "#print(filtered_data.count())\n",
    "      \n",
    "# Convert PSTCO to a 5-character FIPSTCO equivalent to `put(PSTCO,Z5.)`\n",
    "filtered_data = filtered_data.withColumn(\"FIPSTCO\", lpad(col(\"PSTCO\"), 5, \"0\"))\n",
    "#filtered_data = filtered_data.withColumn(\"FIPSTCO\", expr(\"lpad(PSTCO, 5, 0)\"))\n",
    "#map POVCAT based on FIPSTCO\n",
    "povcat = list(POVCAT.keys())\n",
    "filtered_data = filtered_data.withColumn(\"POVCAT\", when(col(\"FIPSTCO\").isin(povcat), mapping_expr[col(\"FIPSTCO\")]))\n",
    "\n",
    "# Apply the mdx and mpr functions\n",
    "output_data = filtered_data\n",
    "output_data.persist(StorageLevel.MEMORY_AND_DISK) # added for optimization\n",
    "#output_data = output_data.repartition(10, 'AGE')\n",
    "output_data = mdx(output_data, \"MYOCARD\", DX_Acute_MyoCard_Infarct_Keys)\n",
    "output_data = mdx(output_data, \"ANEURYSM\", DX_Aneurysm_Keys)\n",
    "output_data = mdx(output_data, \"ACUTE_RENAL\", DX_Acute_Renal_Fail_Keys)\n",
    "output_data = mdx(output_data, \"ACUTE_RENAL3\", DX_Acute_Renal_Fail_Keys) \n",
    "output_data = mpr(output_data, \"ACUTE_RENAL_DIAL\", DIALYIP_Keys) # additinal column for ACUTE_RENAL3\n",
    "output_data = mdx(output_data, \"RESP_DISTRESS\", DX_Acute_Resp_Distress_Keys)\n",
    "output_data = mdx(output_data, \"AM_FLUID\", DX_Amniotic_Fluid_Emb_Keys)\n",
    "output_data = mdx(output_data, \"CARD_ARR\", DX_Card_Arrest_Vent_Fib_Keys)\n",
    "output_data = mpr(output_data, \"CONV_CARD\", PR_Conv_Cardiac_Rhythm_Keys)\n",
    "output_data = mdx(output_data, \"DISS_INTRA\", DX_Diss_Intravasc_Coagul_Keys)\n",
    "output_data = mdx(output_data, \"DISS_INTRA3\", DX_Diss_Intravasc_Coagul3_Keys)\n",
    "output_data = mdx(output_data, \"ECLAMPSIA\", DX_Eclampsia_Keys)\n",
    "output_data = mdx(output_data, \"HEART_FAIL\", DX_Heart_Fail_Surgery_Keys)\n",
    "output_data = mdx(output_data, \"PUERP\", DX_Puerp_Cerebrovascular_Keys)\n",
    "output_data = mdx(output_data, \"PULM_ED\", DX_Pulmonary_Edema_Keys)\n",
    "output_data = mdx(output_data, \"ANES_COMP\", DX_Severe_Anesth_Comp_Keys)\n",
    "output_data = mdx(output_data, \"SEPSIS\", DX_Sepsis_Keys)\n",
    "output_data = mdx(output_data, \"SHOCK\", DX_Shock_Keys)\n",
    "output_data = mdx(output_data, \"SICKLE\", DX_Sickle_Cell_Crisis_Keys)\n",
    "output_data = mdx(output_data, \"AIR_THROMB\", DX_Air_Thrombotic_Embolism_Keys)\n",
    "output_data = mpr(output_data, \"HYSTER\", PR_Hysterectomy_Keys)\n",
    "output_data = mpr(output_data, \"TRACHEO\", PR_Temp_Tracheostomy_Keys)\n",
    "output_data = mpr(output_data, \"VENT\", PR_Ventilation_Keys)\n",
    "output_data = mdx(output_data, \"DX_ABORTION\", DX_Abortion_Keys)\n",
    "output_data = mpr(output_data, \"PR_ABORTION\", PR_Abortion_Keys)\n",
    "\n",
    "output_data = output_data.withColumn(\"DECEASED_FLAG\", when(col(\"DISP\") == 20, 1).otherwise(0))\n",
    "output_data = output_data.withColumn(\"ACUTE_RENAL3\",when((col(\"ACUTE_RENAL3\") == 1) & (col(\"ACUTE_RENAL_DIAL\") == 1), 1).otherwise(None))\n",
    "\n",
    "output_data = output_data.withColumn(\"TAMH01\", when((col(\"DX_ABORTION\") == 1)  | (col(\"PR_ABORTION\") == 1), None).otherwise(greatest(\n",
    "    col(\"MYOCARD\"),\n",
    "    col(\"ANEURYSM\"),\n",
    "    col(\"ACUTE_RENAL\"),\n",
    "    col(\"RESP_DISTRESS\"),\n",
    "    col(\"AM_FLUID\"),\n",
    "    col(\"CARD_ARR\"),\n",
    "    col(\"CONV_CARD\"),\n",
    "    col(\"DISS_INTRA\"),\n",
    "    col(\"ECLAMPSIA\"),\n",
    "    col(\"HEART_FAIL\"),\n",
    "    col(\"PUERP\"),\n",
    "    col(\"PULM_ED\"),\n",
    "    col(\"ANES_COMP\"),\n",
    "    col(\"SEPSIS\"),\n",
    "    col(\"SHOCK\"),\n",
    "    col(\"SICKLE\"),\n",
    "    col(\"AIR_THROMB\"),\n",
    "    col(\"HYSTER\"),\n",
    "    col(\"TRACHEO\"),\n",
    "    col(\"VENT\")))) \\\n",
    "    .withColumn(\"TAMH02\", when((col(\"DX_ABORTION\") == 1)  | (col(\"PR_ABORTION\") == 1), None).otherwise(greatest(\n",
    "    col(\"MYOCARD\"),\n",
    "    col(\"ANEURYSM\"),\n",
    "    col(\"ACUTE_RENAL\"),\n",
    "    col(\"RESP_DISTRESS\"),\n",
    "    col(\"AM_FLUID\"),\n",
    "    col(\"CARD_ARR\"),\n",
    "    col(\"CONV_CARD\"),\n",
    "    col(\"DISS_INTRA\"),\n",
    "    col(\"ECLAMPSIA\"),\n",
    "    col(\"HEART_FAIL\"),\n",
    "    col(\"PUERP\"),\n",
    "    col(\"PULM_ED\"),\n",
    "    col(\"ANES_COMP\"),\n",
    "    col(\"SEPSIS\"),\n",
    "    col(\"SHOCK\"),\n",
    "    col(\"SICKLE\"),\n",
    "    col(\"AIR_THROMB\"),\n",
    "    col(\"HYSTER\"),\n",
    "    col(\"TRACHEO\"),\n",
    "    col(\"VENT\"),\n",
    "    col(\"DECEASED_FLAG\")))) \\\n",
    "    .withColumn(\"TAMH03\", when((col(\"DX_ABORTION\") == 1)  | (col(\"PR_ABORTION\") == 1), None).otherwise(greatest(\n",
    "    col(\"MYOCARD\"),\n",
    "    col(\"ANEURYSM\"),\n",
    "    col(\"ACUTE_RENAL3\"),\n",
    "    col(\"RESP_DISTRESS\"),\n",
    "    col(\"AM_FLUID\"),\n",
    "    col(\"CARD_ARR\"),\n",
    "    col(\"CONV_CARD\"),\n",
    "    col(\"DISS_INTRA3\"),\n",
    "    col(\"ECLAMPSIA\"),\n",
    "    col(\"HEART_FAIL\"),\n",
    "    col(\"PUERP\"),\n",
    "    col(\"PULM_ED\"),\n",
    "    col(\"ANES_COMP\"),\n",
    "    col(\"SEPSIS\"),\n",
    "    col(\"SHOCK\"),\n",
    "    col(\"SICKLE\"),\n",
    "    col(\"AIR_THROMB\"),\n",
    "    col(\"HYSTER\"),\n",
    "    col(\"TRACHEO\"),\n",
    "    col(\"VENT\"),\n",
    "    col(\"DECEASED_FLAG\"))))\n",
    "\n",
    "#print('Calculation Complete')\n",
    "\n",
    "#write output and stop spark session\n",
    "#output_data.coalesce(1).write.mode(\"overwrite\").csv(\"/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/sid_2021_10K-preprocessed-updated.csv\", header=True)\n",
    "print(f\"Total Execution Time Part 3: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca78ad8-0b6c-4fb7-9053-76b052691a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#This is code aggregate the code by columns\n",
    "def report_aggregations(\n",
    "    input_data: DataFrame, \n",
    "    group_columns: List[str], \n",
    "    measure_columns: List[str]\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Efficiently performs multiple aggregation operations across different grouping levels.\n",
    "    \n",
    "    This function optimizes performance by:\n",
    "    1. Using broadcast variables for small dimensions\n",
    "    2. Caching intermediate results to avoid recomputation\n",
    "    3. Using parallel execution where possible\n",
    "    4. Optimizing the aggregation pipeline\n",
    "    \n",
    "    Args:\n",
    "        input_data: The Spark DataFrame to aggregate\n",
    "        group_columns: Columns to group by in separate aggregations\n",
    "        measure_columns: Columns to compute statistics on\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all aggregations combined\n",
    "    \"\"\"\n",
    "    # Cache the input data to avoid recomputation\n",
    "    output_data = input_data.cache()\n",
    "    \n",
    "    # Create aggregation expressions\n",
    "    sum_exprs = [spark_sum(mcol).alias(mcol) for mcol in measure_columns]\n",
    "    count_exprs = [count(mcol).alias(f\"P{mcol[1:]}\") for mcol in measure_columns]\n",
    "    mean_exprs = [mean(mcol).alias(f\"O{mcol[1:]}\") for mcol in measure_columns]\n",
    "    all_exprs = sum_exprs + count_exprs + mean_exprs\n",
    "    \n",
    "    # Function to compute a single aggregation\n",
    "    def compute_aggregation(grouping_cols=None):\n",
    "        if grouping_cols:\n",
    "            if not isinstance(grouping_cols, list):\n",
    "                grouping_cols = [grouping_cols]\n",
    "            return output_data.groupBy(*grouping_cols).agg(*all_exprs)\n",
    "        else:\n",
    "            return output_data.agg(*all_exprs)\n",
    "    \n",
    "    # Approach 1: Using concurrent.futures for parallel execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=min(8, len(group_columns) + 1)) as executor:\n",
    "        # Submit global summary task\n",
    "        future_global = executor.submit(compute_aggregation)\n",
    "        \n",
    "        # Submit individual column grouping tasks\n",
    "        future_to_col = {executor.submit(compute_aggregation, col): col for col in group_columns}\n",
    "        \n",
    "        # Collect results\n",
    "        result_dfs = []\n",
    "        \n",
    "        # Get global summary result\n",
    "        try:\n",
    "            global_df = future_global.result()\n",
    "            # Add null values for grouping columns to global summary\n",
    "            for col_name in group_columns:\n",
    "                global_df = global_df.withColumn(col_name, lit(None))\n",
    "            result_dfs.append(global_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Global aggregation failed: {e}\")\n",
    "            \n",
    "        # Get column grouping results\n",
    "        for future in concurrent.futures.as_completed(future_to_col):\n",
    "            col_name = future_to_col[future]\n",
    "            try:\n",
    "                col_df = future.result()\n",
    "                # Add null values for other grouping columns\n",
    "                for other_col in [c for c in group_columns if c != col_name]:\n",
    "                    col_df = col_df.withColumn(other_col, lit(None))\n",
    "                result_dfs.append(col_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Aggregation for {col_name} failed: {e}\")\n",
    "    \n",
    "    \n",
    "    # Use unionByName with allowMissingColumns=True to handle different schemas\n",
    "    final_result = reduce(\n",
    "        lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), \n",
    "        result_dfs\n",
    "    )\n",
    "    \n",
    "    # Apply scalar multiplications all at once instead of sequentially\n",
    "    for col_name in [\"OAMH01\", \"OAMH02\", \"OAMH03\"]:\n",
    "        if col_name in final_result.columns:\n",
    "            final_result = final_result.withColumn(col_name, col(col_name) * lit(10000))\n",
    "    \n",
    "    # Sort the result once at the end\n",
    "    # Use a list of columns that actually exist in the result\n",
    "    existing_sort_columns = [c for c in group_columns if c in final_result.columns]\n",
    "    sorted_result = final_result.orderBy(*existing_sort_columns)\n",
    "    \n",
    "    # Cache the final result\n",
    "    sorted_result = sorted_result.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    return sorted_result\n",
    "\n",
    "# Execute the optimized function with your parameters\n",
    "group_columns = key_columns#[\"PAYCAT\", \"YEAR\", \"HOSPST\", \"POVCAT\", \"RACECAT\"]\n",
    "measure_columns = [\"TAMH01\", \"TAMH02\", \"TAMH03\"]\n",
    "\n",
    "# Run the optimized function\n",
    "result = report_aggregations(output_data, group_columns, measure_columns)\n",
    "\n",
    "# Show the result\n",
    "#result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a07bf0-ee63-4fc9-9318-99f044a7e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.coalesce(1).write.mode(\"overwrite\").csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c336b-3642-44a3-be1f-f7f90bad811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# # Replace these with your actual column names and values\n",
    "# #custom_stratum = \"your_custom_stratum_column\"  # Replace with actual column name\n",
    "# group_columns = [\"PAYCAT\", \"YEAR\", \"HOSPST\", \"POVCAT\", \"RACECAT\"]\n",
    "# measure_columns = [\"TAMH01\", \"TAMH02\", \"TAMH03\"]\n",
    "\n",
    "# # Create a list to store all the aggregation results\n",
    "# result_dfs = []\n",
    "\n",
    "# # Create aggregation expressions for sum, count, and mean\n",
    "# sum_exprs = [spark_sum(mcol).alias(mcol) for mcol in measure_columns]\n",
    "# count_exprs = [count(mcol).alias(f\"P{mcol[1:]}\") for mcol in measure_columns]\n",
    "# mean_exprs = [mean(mcol).alias(f\"O{mcol[1:]}\") for mcol in measure_columns]\n",
    "\n",
    "# # Combine all expressions\n",
    "# all_exprs = sum_exprs + count_exprs + mean_exprs\n",
    "\n",
    "# # First do the global summary (ways 0)\n",
    "# global_summary = output_data.agg(*all_exprs)\n",
    "\n",
    "# result_dfs.append(global_summary)\n",
    "\n",
    "# # Then do individual column groupings (ways 1)\n",
    "# for group_column in group_columns:\n",
    "#     col_summary = output_data.groupBy(group_column).agg(*all_exprs)\n",
    "#     result_dfs.append(col_summary)\n",
    "\n",
    "# # Union all the results\n",
    "# final_result = result_dfs[0]\n",
    "# for i in range(1, len(result_dfs)):\n",
    "#     final_result = final_result.unionByName(result_dfs[i], allowMissingColumns=True)\n",
    "\n",
    "# # Sort the result by all the grouping columns\n",
    "# sorted_result = final_result.orderBy(\n",
    "#      \"PAYCAT\", \"YEAR\", \"HOSPST\", \"POVCAT\", \"RACECAT\"\n",
    "# )\n",
    "\n",
    "# # for measure_col in measure_columns:\n",
    "# #     sorted_result = sorted_result.filter(sorted_result[measure_col].isNotNull())\n",
    "# #     # For empty strings\n",
    "# #     sorted_result = sorted_result.filter(sorted_result[measure_col] != \"\")\n",
    "\n",
    "# # sorted_result.show()\n",
    "\n",
    "# sorted_result = sorted_result.withColumn(\"OAMH01\", col(\"OAMH01\") * lit(10000))\n",
    "# sorted_result = sorted_result.withColumn(\"OAMH02\", col(\"OAMH02\") * lit(10000))\n",
    "# sorted_result = sorted_result.withColumn(\"OAMH03\", col(\"OAMH03\") * lit(10000))\n",
    "\n",
    "# #print(sorted_result)\n",
    "# sorted_result.show()\n",
    "# Save the sorted result\n",
    "#sorted_result.coalesce(1).write.mode(\"overwrite\").csv(f\"/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/MHI-report-{data_size}.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397f392-1796-47ef-89c4-664128345464",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time=time.time()\n",
    "print(f\"Total Execution Time: {end_time - start_time}\")\n",
    "\n",
    "#sas_result_file=f\"MHAO_v2024_21_{data_size}.txt\"\n",
    "#/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA\n",
    "# sas_outcome = spark.read.option(\"header\", True).csv(f\"{sas_result_file}\")#spark.read.option(\"header\", True).csv(f\"{data_path}/{sas_result_file}\")\n",
    "\n",
    "# # Remove extra spaces from all string columns\n",
    "# for col_name in sas_outcome.columns:\n",
    "#     sas_outcome = sas_outcome.withColumn(col_name, trim(col(col_name)))  # Remove leading/trailing spaces\n",
    "\n",
    "\n",
    "# column_mapping = {\n",
    "#     \"Race\": \"RACECAT\",\n",
    "#     \"Poverty\": \"POVCAT\",\n",
    "#     \"State\": \"HOSPST\",\n",
    "#     \"Year\":\"YEAR\",\n",
    "#     \"Payer\":\"PAYCAT\",\n",
    "#     \"TAMH01\":\"TAMH01\",\n",
    "#     \"TAMH02\":\"TAMH02\",\n",
    "#     \"TAMH03\":\"TAMH03\",\n",
    "#     \"PAMH01\":\"PAMH01\",\n",
    "#     \"PAMH02\":\"PAMH02\",\n",
    "#     \"PAMH03\":\"PAMH03\",\n",
    "#     \"OAMH01\":\"OAMH01\",\n",
    "#     \"OAMH02\":\"OAMH02\",\n",
    "#     \"OAMH03\":\"OAMH03\"\n",
    "# }\n",
    "\n",
    "# # Rename columns in the text DataFrame using the dictionary\n",
    "# for old_col, new_col in column_mapping.items():\n",
    "#     sas_outcome = sas_outcome.withColumnRenamed(old_col, new_col)\n",
    " \n",
    "# # columns to compare\n",
    "# non_key_columns = [col for col in result.columns if col not in key_columns]\n",
    "\n",
    "# null_replacements = [\".\"]  # Define text values to treat as null\n",
    "\n",
    "# for col_name in key_columns:\n",
    "#     sas_outcome = sas_outcome.withColumn(col_name, when(col(col_name) == \".\", \"\").otherwise(col(col_name)))\n",
    "#     result = result.withColumn(col_name, when(col(col_name).isNull() , \"\").otherwise(col(col_name)))\n",
    "\n",
    "# # result.show()\n",
    "# # sas_outcome.show()\n",
    "\n",
    "# # Perform inner join on key columns\n",
    "# combined_outcome = result.alias(\"result\").join(sas_outcome.alias(\"ref\"), key_columns, \"inner\")\n",
    "\n",
    "# #combined_outcome.show()\n",
    "\n",
    "# # Compare non-key columns\n",
    "# for col_name in non_key_columns:\n",
    "#     combined_outcome = combined_outcome.withColumn(\n",
    "#         f\"{col_name}_match\",\n",
    "#         when(round(col(f\"result.{col_name}\"),5) == round(col(f\"ref.{col_name}\"),5), lit(1)).otherwise(lit(0))\n",
    "#     )\n",
    "\n",
    "# # combined_outcome.show()\n",
    "# # combined_outcome.write.mode(\"overwrite\").csv(f\"/Users/mshaque/Workarea/Projects/qi-pyspark-poc/DATA/Comparison-{data_size}.csv\", header=True)\n",
    "\n",
    "# # Calculate accuracy by #rows\n",
    "# total_records = combined_outcome.count()\n",
    "# matching_records = combined_outcome.filter(expr(\" and \".join([f\"{col}_match = 1\" for col in non_key_columns]))).count()\n",
    "# accuracy = (matching_records / total_records) * 100 if total_records > 0 else 0\n",
    "\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.2f}% ({matching_records}/{total_records})\")\n",
    "\n",
    "# # Calculate Accuracy by data points\n",
    "# total_data_points = combined_outcome.count() * len(non_key_columns)\n",
    "# matching_data_points = combined_outcome.select([spark_sum(col(f\"{col_header}_match\")) for col_header in non_key_columns]).collect()[0]\n",
    "# matching_data_points = sum(matching_data_points)\n",
    "\n",
    "# accuracy = (matching_data_points / total_data_points) * 100 if total_data_points > 0 else 0\n",
    "# print(f\"Accuracy by Data Points: {accuracy:.2f}% ({matching_data_points}/{total_data_points})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53818928-b0d6-4f4a-a468-e8adb5a8b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339352ca-0e68-4fc1-845e-e5a921aa1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "print(execution_time_output_path)\n",
    "fexecution = open(execution_time_output_path, \"w\")\n",
    "fexecution.write(str(total_time * 1000))\n",
    "fexecution.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
